<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: OpenCV | Eaglesky's Blog]]></title>
  <link href="http://eaglesky.github.io/blog/categories/opencv/atom.xml" rel="self"/>
  <link href="http://eaglesky.github.io/"/>
  <updated>2014-03-28T09:37:15-07:00</updated>
  <id>http://eaglesky.github.io/</id>
  <author>
    <name><![CDATA[Yalun Qin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Color-Based Hand Gesture Recognition on Android]]></title>
    <link href="http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android/"/>
    <updated>2014-03-27T14:40:57-07:00</updated>
    <id>http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android</id>
    <content type="html"><![CDATA[<p>This is my course project for CS290I in UCSB. The aim is to detect and recognize
user-defined gestures using the camera of Android phone. A simple
demo that let user call other applications using gestures is developed to show the real time
performance.</p>

<!-- more -->

<h2 id="overview">Overview</h2>

<p><img class="center" src="/images/posts/hand-gesture/workflow.jpg" width="500" height="310" title="Architecture for Hand Gesture Recognition on Android" ></p>

<h2 id="presampling-and-hand-segmentation">Presampling and Hand Segmentation</h2>
<p><img class="left" src="/images/posts/hand-gesture/presampled-back.jpg" width="380"> <img class="left" src="/images/posts/hand-gesture/presampled-hand.jpg" width="380"></p>

<p>The app starts with two pre-sampling steps, which collect colors of background and the user’s hand using 7 small squares displayed on the screen. These color data are used to compute the thresholds to get the binary image
from the input RGBA data. For simplicity, adaptive method was not used to find
the best threshold. Instead, the user is asked to put his hand close to the screen
to cover the 7 squares so that the  program could get the color data of the
hand. Note that the number of squares is just an empirical value and it could
be other value. After the 7 color data for the hand are obtained , 7
upper and lower boundaries for the hand area are computed, which
can be represented as a 6 dimensional vector. The bounding vector is determined
by the developer and could not be changed. Therefore, the performance of the
segmentation actually depends on the choice of the bounding vector. To better
understand the influence of each element of the vector on the segmentation
performance, the original RGB color space is converted to many other color
spaces like HSV, YCrCb, CIE L*a*b*. Since for the same hand, the color usually
varies most on the lighting dimension and smaller on hue and other color
dimensions, the desirable color space should be able to separate them. After
some experiments the three color spaces listed are all found to perform
quite well, but it is easier to find the best bounding vector by using
Lab space. Therefore this color space is chosen.</p>

<p><img class="center" src="/images/posts/hand-gesture/thresholding-back.jpg" width="500"></p>

<p>After the boundaries for each of the 7 color data are obtained, 7 binary images of the hand can be computed, which are then summed up using logical operation “or”. The same thing are done on the background color
, producing another binary image. The logical operation “and” is then done on
the two binary images and the result is blurred, dilated and eroded to eliminate noises, and the final binary image is obtained, as is shown in the above picture.</p>

<h2 id="feature-extraction">Feature Extraction</h2>

<p><span class='caption-wrapper left'><img class='caption' src='/images/posts/hand-gesture/features.jpg' width='352' height='288' title='Illustration of the elements needed to calculate features. Green box represents the bounding box of the hand. Blue line represents the convex hull. Red line is the contour of the hand. Green dots between the fingers are defect points. The yellow circle is the inscribed circle and the black dot is the center of the circle, which is treated as the center of the palm. Dark green lines are finger vectors, and the order is indicted by the numbers.'><span class='caption-text'>Illustration of the elements needed to calculate features. Green box represents the bounding box of the hand. Blue line represents the convex hull. Red line is the contour of the hand. Green dots between the fingers are defect points. The yellow circle is the inscribed circle and the black dot is the center of the circle, which is treated as the center of the palm. Dark green lines are finger vectors, and the order is indicted by the numbers.</span></span></p>

<p>The features used to represent the hand are low-level features, which
essentially are the finger vectors. To compute them, it is necessary to find out the
locations of the palm center and the fingertips. Some of the OpenCV
functions, can directly return the contour and convex hull points of the hand.
Using these coordinates and the bounding box, the center and
radius of the inscribed circle of the contour can be computed. This part is quite
computationally expensive and thus it is implemented in native C++ code. </p>

<p><img class="center" src="/images/posts/hand-gesture/defects-elimination" width="400">
The locations of the fingertips are computed using the defect points. To do
that, redundant defect points are eliminated by checking constraints on the depth and angle of the defect points and so on, as is shown in the above picture. Next the defect points left are reordered and the finger tips can be obtained from the
returned coordinates of OpenCV functions. Finally the finger vectors are
computed and divided by the radius of inscribe circle to get the final
feature vector:</p>

]]></content>
  </entry>
  
</feed>
