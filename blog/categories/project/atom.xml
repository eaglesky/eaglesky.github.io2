<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Project | Eaglesky's Blog]]></title>
  <link href="http://eaglesky.github.io/blog/categories/project/atom.xml" rel="self"/>
  <link href="http://eaglesky.github.io/"/>
  <updated>2014-03-28T14:58:09-07:00</updated>
  <id>http://eaglesky.github.io/</id>
  <author>
    <name><![CDATA[Yalun Qin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Color-Based Hand Gesture Recognition on Android]]></title>
    <link href="http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android/"/>
    <updated>2014-03-27T14:40:57-07:00</updated>
    <id>http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android</id>
    <content type="html"><![CDATA[<p>This is my course project for CS290I in UCSB. The aim is to detect and recognize
user-defined gestures using the camera of Android phone. A simple
demo that let user call other applications using gestures is developed to show the real time
performance.</p>

<!-- more -->

<h2 id="overview">Overview</h2>

<p><img class="center" src="/images/posts/hand-gesture/workflow.jpg" width="500" height="310" title="Architecture for Hand Gesture Recognition on Android" ></p>

<h2 id="presampling-and-hand-segmentation">Presampling and Hand Segmentation</h2>
<p><img class="left" src="/images/posts/hand-gesture/presampled-back.jpg" width="380"> <img class="left" src="/images/posts/hand-gesture/presampled-hand.jpg" width="380"></p>

<p>The app starts with two pre-sampling steps, which collect colors of background and the user’s hand using 7 small squares displayed on the screen. These color data are used to compute the thresholds to get the binary image
from the input RGBA data. For simplicity, adaptive method was not used to find
the best threshold. Instead, the user is asked to put his hand close to the screen
to cover the 7 squares so that the  program could get the color data of the
hand. Note that the number of squares is just an empirical value and it could
be other value. After the 7 color data for the hand are obtained , 7
upper and lower boundaries for the hand area are computed, which
can be represented as a 6 dimensional vector. The bounding vector is determined
by the developer and could not be changed. Therefore, the performance of the
segmentation actually depends on the choice of the bounding vector. To better
understand the influence of each element of the vector on the segmentation
performance, the original RGB color space is converted to many other color
spaces like HSV, YCrCb, CIE L*a*b*. Since for the same hand, the color usually
varies most on the lighting dimension and smaller on hue and other color
dimensions, the desirable color space should be able to separate them. After
some experiments the three color spaces listed are all found to perform
quite well, but it is easier to find the best bounding vector by using
Lab space. Therefore this color space is chosen.</p>

<p><img class="center" src="/images/posts/hand-gesture/thresholding-back.jpg" width="500"></p>

<p>After the boundaries for each of the 7 color data are obtained, 7 binary images of the hand can be computed, which are then summed up using logical operation “or”. The same thing are done on the background color
, producing another binary image. The logical operation “and” is then done on
the two binary images and the result is blurred, dilated and eroded to eliminate noises, and the final binary image is obtained, as is shown in the above picture.</p>

<h2 id="feature-extraction">Feature Extraction</h2>

<p><span class='caption-wrapper left'><img class='caption' src='/images/posts/hand-gesture/features.jpg' width='352' height='288' title='Illustration of the elements needed to calculate features. Green box represents the bounding box of the hand. Blue line represents the convex hull. Red line is the contour of the hand. Green dots between the fingers are defect points. The yellow circle is the inscribed circle and the black dot is the center of the circle, which is treated as the center of the palm. Dark green lines are finger vectors, and the order is indicted by the numbers.'><span class='caption-text'>Illustration of the elements needed to calculate features. Green box represents the bounding box of the hand. Blue line represents the convex hull. Red line is the contour of the hand. Green dots between the fingers are defect points. The yellow circle is the inscribed circle and the black dot is the center of the circle, which is treated as the center of the palm. Dark green lines are finger vectors, and the order is indicted by the numbers.</span></span></p>

<p>The features used to represent the hand are low-level features, which
essentially are the finger vectors. To compute them, it is necessary to find out the
locations of the palm center and the fingertips. Some of the OpenCV
functions can directly return the contour and convex hull points of the hand
from a binary image containing the segmented hand. Using these coordinates and the bounding box, the center and
radius of the inscribed circle of the contour can be computed. This part is quite
computationally expensive and thus it is implemented in native C++ code. </p>

<p><img class="center" src="/images/posts/hand-gesture/defects-elimination.jpg" width="400"></p>

<p>The locations of the fingertips are computed using the defect points. To do
so,  redundant defect points are eliminated by checking constraints on the depth and angle of the defect points and so on, as is shown in the above picture. Next the defect points left are reordered and the finger tips can be obtained from the
returned coordinates of OpenCV functions. Finally the finger vectors are
computed and divided by the radius of inscribe circle to get the final
feature vector: <script type="math/tex">X = [x_0 / r, y_0 / r, x_1 / r, y_1/r ... x_4/r, y_4/r] </script>.</p>

<h2 id="training-set">Training Set</h2>

<p><img class="center" src="/images/posts/hand-gesture/training-set.jpg" width="500"></p>

<p>Currently 19 gestures have been collected to constitute the initial data
set, as is shown in above figure. Note that although the extracted feature vector is
invariant to different size of hand, it is variant to different orientations
since gestures of different orientations are treated as different gestures. Also
it can be seen from above figure that the “gesture” labeled as 0 is actually not a
gesture. The label 0 is reserved to represent anything whose features are not
detected. It can be counted as representing a negative example. It also reveals
one of the limitations that the “fist” gesture can not be recognized. </p>

<p>The user can add whatever gestures they like to the initial data set or build
another one using the application. The training process can be conducted in
any conditions as long as the hand can be well detected. Due to time
constraint, very few number of frames are collected for each gesture(Only
10 frames). Multi-class SVM is used to build the training model and make
predictions.</p>

<h2 id="results-and-analysis">Results and Analysis</h2>

<p>To show the results, several tests were conducted in different lighting conditions and
different backgrounds. For each gesture, 3 frames were captured together with the
predicted labels shown as the red numbers in the photos. Three frames are far
less than enough to prove the accuracy. However according to observation,
the final recognition result highly depends on the detection result. In other
words, if the hand is well-detected, which means the extracted feature vector
correctly represents the hand gesture, then the hand can be classified into the
right category with high probability. Therefore what really matters is actually the
performance of hand detection rather than that of hand recognition. The performance of
hand detection can be seen from the pictures captured, which display elements
like the contour and finger vectors. Three frames are enough to see if these
elements are correct and if they are stable or not.</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/posts/hand-gesture/s1.jpg' width='600' height='691' title='Results in Scenario 1'><span class='caption-text'>Results in Scenario 1</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/posts/hand-gesture/s2.jpg' width='600' height='691' title='Results in Scenario 2'><span class='caption-text'>Results in Scenario 2</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/posts/hand-gesture/s3.jpg' width='600' height='691' title='Results in Scenario 3'><span class='caption-text'>Results in Scenario 3</span></span></p>

<p>The app was tested in three scenarios and the results are shown in above
figures. It can be seen that the app worked quite well in Scenario 1 and Scenario 3: all the predicted labels are correct and most of the gestures are detected correctly and quite stable, except there is one gesture
not detected(Random error). And the main reason for the success is that there
are sharp contrast in color between the background and the hand. Whether the
background is cluttered or not does not matter, only the color matters.</p>

<p>In Scenario 2 some of the gestures are not well-detected. The main cause for
 this is that the yellow color in the background kind of interfere the extraction of contour of the hand. Also the auto adjustment of the camera became stronger in this scenario which is probably because of the
 light.</p>

<p>In summary, with the help of presampled colors, the app can detect and
 recognize the hand quite well and stable in most of the lighting conditions
 and backgrounds, as long as the contrast of the background and the hand is not
 too poor. However the constraint is that they must keep the same once the
 presample processes finish. And that means both of the hand and the phone
 should not move too much, in order to avoid the change of the colors.</p>

<h2 id="demo">Demo</h2>

<p>
<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('PF6hY-0VuN4');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/PF6hY-0VuN4?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/PF6hY-0VuN4/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=PF6hY-0VuN4" id="PF6hY-0VuN4" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">Hand gesture recognition using OpenCV on Android</div>
</a>
<div class="video-info" >This is an app that can recognize hand gestures and let user call other apps using gestures. It basically uses color-based methods to detect the gestures and then classify them using SVM classifier. As you can see from the video, there are 18 gestures that can be recognized, and the gesture database can be further extended by the user.  This app works pretty well in many different scenarios as long as the contrast is not too poor. However,  change of lighting conditions and background colors will affect the performance. So the user must make sure the lighting condition is stable and the background doesn't have colors very closed to the hand.

Report and code and be found at: http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android/</div>
</div>

</p>

<h2 id="summary">Summary</h2>

<h3 id="potential-improvements">Potential Improvements</h3>
<ol>
  <li>Build statistical model for the color of background and hand instead of
simple sampling and summing.</li>
  <li>Use adaptive thresholding to get the final binary image.</li>
  <li>Add elimination process of skin-like parts like wrist, forearm and face.</li>
  <li>Add feature representation for the “fist” gesture.</li>
  <li>Make prediction according to probability, rather than directly output the
prediction frame by frame. </li>
</ol>

<h3 id="limitations">Limitations</h3>

<p>Two most evident drawbacks mentioned before are influences of change in
background colors and lighting conditions. One thing I found out is that the
camera on the phone has auto-adjustment like auto-exposure and auto white
balancing which change the color of the hand and background as the view or
position of the hand changes. This makes color-based methods not desirable to
be used on mobile phones. The only solution is either find ways to disable the auto adjustment, which I think is not possible on most of the devices, or abandon the methods and instead use hand detection methods
based on texture-like features together with some machine learning methods.</p>

<p>In addition, since low-level features are used to represent the gesture, some
gestures can not be represented. Also it is hard to eliminate false-positives.</p>

<h3 id="source-code">Source Code</h3>

<p><a href="https://github.com/eaglesky/HandGestureApp">Here!</a></p>

<h2 id="acknowledgments">Acknowledgments</h2>

<ol>
  <li>The idea of presampling is from Simen Andresen’s <a href="http://simena86.github.io/blog/2013/08/12/hand-tracking-and-recognition-with-opencv/">blog</a> </li>
  <li>Used LibSVM on Android implemented by Kun Li.
<a href="https://github.com/cnbuff410/Libsvm-androidjni">link</a></li>
  <li>Used <a href="https://github.com/iPaulPro/aFileChooser">aFileChooser library</a> to
implement the file chooser.</li>
  <li>To implement mapping human gestures to Android apps, <a href="http://blog.csdn.net/qinjuning/article/details/6867806">this article</a> is quite helpful.</li>
</ol>

]]></content>
  </entry>
  
</feed>
