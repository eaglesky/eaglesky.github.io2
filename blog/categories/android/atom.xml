<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Android | Eaglesky's Blog]]></title>
  <link href="http://eaglesky.github.io/blog/categories/android/atom.xml" rel="self"/>
  <link href="http://eaglesky.github.io/"/>
  <updated>2014-03-27T21:25:13-07:00</updated>
  <id>http://eaglesky.github.io/</id>
  <author>
    <name><![CDATA[Yalun Qin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Color-Based Hand Gesture Recognition on Android]]></title>
    <link href="http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android/"/>
    <updated>2014-03-27T14:40:57-07:00</updated>
    <id>http://eaglesky.github.io/blog/2014/03/27/color-based-hand-gesture-recognition-on-android</id>
    <content type="html"><![CDATA[<p>This is my course project for CS290I in UCSB. The aim is to detect and recognize
user-defined gestures using the camera of Android phone. I developed a simple
demo that let user call other applications using gestures to show the real time
performance.</p>

<!-- more -->

<h2 id="introduction">Introduction</h2>

<p>Nowadays mobile phones have been the most widely used and must-have electronic
gadgets. In recent years, the way how users interact with mobile phones has
evolved from QWERTY keyboards to touch screens. As is predicted by many people,
touch-free technologies like hand gesture recognition could probably be the
next user interface for mobile phones. This new interface gives a whole new way
to users for interacting with their phones, and in some scenarios can make
interaction easier than other ways. One possible situation is controlling the
play of music in the phone remotely. This can be achieved by using gesture
recognition as long as the hand is in the view of the camera. Another situation
is interacting with the phone when using the touch screen is not desirable. For
example, when the user is eating, his hands are probably dirty and greasy. It
will be much better and safer for the screen if the user can still use the
phone without touching it.</p>

<p>My project aims to detect hand gestures using the camera of Android phone and
recognize them using SVM classifier. Note that I only consider the static
gestures, which essentially are hand postures. The app I developed also maps
those gestures to some other pre-installed applications on the phone, enabling
the user to launch any of them by doing the corresponding gesture.</p>

<h2 id="methodology">Methodology</h2>

<h3 id="overview">Overview</h3>

<p><span class='caption-wrapper'><img class='caption' src='/images/posts/hand-gesture/workflow.jpg' width='500' height='310' title='Figure 2.1 Overview'><span class='caption-text'>Figure 2.1 Overview</span></span></p>

<p>My app starts with two pre-sampling steps, which collecting colors of
background and the user’s hand using 7 small squares displayed on the screen.
These color data will be used to compute the threshold to get the binary image
from the input RGBA data. Since it is a constant, the color of user’s hand and
the background must be stable in order to make sure the hand could be well
segmented. Therefore stable colors is the first assumption I made. </p>

<p>After the binary image is obtained, I can extract the contours in it. To make sure
the major contour comes from the hand, I made our second assumption that there
are no other objects which have similar color with the user’s hand. And this
means that user’s face must not be in the view of the camera. I believe it
won’t cause much inconvenience to the user since the view of  the camera on
the phone is actually quite small compared to those cameras on other devices.
Also because of the portability of the phone, user can easily adjust the angle
so that his face will not be in the view of the camera.</p>

<p>By using some OpenCV functions, I can get the coordinates of palm center and
fingertips from the contour and convex hull. Then I can compute the feature
vector from these coordinates. The feature vector representation is essentially
a normalized skeleton of the hand. By collecting the data of 18 gestures, I
built a initial database and generated a SVM training model for testing(Figure
3.1).</p>

]]></content>
  </entry>
  
</feed>
